{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Box, Discrete, MultiDiscrete\n",
    "import os\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random\n",
    "from tensorflow.keras.layers import Input\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from scipy.optimize import minimize\n",
    "import itertools\n",
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "import mplfinance as mpf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ohlc_data(means, correlation_matrix, num_days=5040, num_timesteps=24):\n",
    "    column_names = [f'Stock_{i+1}_{col}' for i in range(len(means)) for col in ['Open', 'High', 'Low', 'Close']]\n",
    "    ohlc_data = []\n",
    "    \n",
    "    # Fill the first row with ones\n",
    "    ohlc_data.append([1] * len(column_names))\n",
    "\n",
    "    for day in range(num_days):\n",
    "        # Generate data for one day\n",
    "        a = st.multivariate_normal.rvs(means, correlation_matrix, size=(1, num_timesteps))\n",
    "        a = np.exp(a)\n",
    "        lity = []\n",
    "        #print(a)\n",
    "        \n",
    "        for timestep in range(len(a[0])):\n",
    "            close_prev = ohlc_data[-1][3 + timestep * 4]\n",
    "            #print(np.cumsum(a[:,timestep]) , close_prev)\n",
    "            \n",
    "            lity.append(close_prev)\n",
    "            lity.append(close_prev * max(np.cumprod(a[:, timestep])))  # High\n",
    "            lity.append(close_prev * min(np.cumprod(a[:, timestep])))  # Low\n",
    "            lity.append(close_prev * np.cumprod(a[:, timestep])[-1])   # Close\n",
    "        #print(lity)\n",
    "        \n",
    "        ohlc_data.append((lity))\n",
    "    \n",
    "    return pd.DataFrame(ohlc_data, columns=column_names)\n",
    "\n",
    "means = [0.00003, 0.00002, 0.000035]\n",
    "correlation_matrix = np.array([[0.000034, 0.0000075, -0.00001],\n",
    "                               [0.0000075, 0.000012, -0.000008],\n",
    "                               [-0.00001, -0.000008, 0.000045]\n",
    "])\n",
    "\n",
    "\n",
    "OHLC = generate_ohlc_data(means=means, correlation_matrix=correlation_matrix)\n",
    "OHLC = OHLC.drop(OHLC.index[0]).reset_index(drop=True)\n",
    "\n",
    "OHLC.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StandardizeData(data, n):\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        df = data.copy()  # Make a copy to avoid modifying the original DataFrame\n",
    "    elif isinstance(data, list):\n",
    "        df = pd.DataFrame(data)  # Convert list to DataFrame\n",
    "    else:\n",
    "        raise ValueError(\"Input data must be a DataFrame or a list.\")\n",
    "    \n",
    "    Standardized = pd.DataFrame()\n",
    "    for col in df.columns:\n",
    "        ST = pd.DataFrame()\n",
    "        for i in range(n+1):\n",
    "            stock_num = int(col.split('_')[1])  # Extract the stock number from the column name\n",
    "            close_col = f'Stock_{stock_num}_Close'  # Get the corresponding 'Close' column\n",
    "            ST[f'{col}_{i}'] = df[col].shift(-i) / df[close_col].shift(-n)  # Divide by the corresponding 'Close' column\n",
    "        Standardized = pd.concat([Standardized, ST], axis=1)\n",
    "    \n",
    "    # Drop rows with NaN values\n",
    "    \n",
    "\n",
    "\n",
    "    Ratio = pd.DataFrame()\n",
    "    close_columns = [col for col in df.columns if 'Close' in col]\n",
    "    for col in close_columns:\n",
    "        \n",
    "        Ratio[col] = np.log(df[col].shift(-n-1) /df[col].shift(-n) )\n",
    "        Ratio.dropna()\n",
    "\n",
    "\n",
    "    return Standardized[:-n-1], Ratio[:-n-1]\n",
    "\n",
    "InputStates, RewardVector = StandardizeData(OHLC, 4)\n",
    "print(RewardVector.iloc[:2529].cov() , RewardVector.iloc[:2529].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tangency Portfolio calculations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tangency_weights(covariance_matrix, mean_vector):\n",
    "    n = len(mean_vector)\n",
    "    \n",
    "    # Objective function to maximize (Sharpe ratio)\n",
    "    def objective_function(weights):\n",
    "        portfolio_return = np.dot(weights, mean_vector)\n",
    "        portfolio_std = np.sqrt(np.dot(weights.T, np.dot(covariance_matrix, weights)))\n",
    "        return -portfolio_return / portfolio_std\n",
    "    \n",
    "    # Constraint: sum of weights equals 1\n",
    "    constraints = ({'type': 'eq', 'fun': lambda weights: np.sum(np.abs(weights)) - 1})\n",
    "    \n",
    "    # Bounds for each weight (-1 <= weight <= 1)\n",
    "    bounds = tuple((-1, 1) for _ in range(n))\n",
    "    \n",
    "    # Initial guess for weights\n",
    "    initial_guess = np.ones(n) / n\n",
    "    \n",
    "    # Optimization\n",
    "    result = minimize(objective_function, initial_guess, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    \n",
    "    # Extracting optimal weights\n",
    "    optimal_weights = result.x\n",
    "    \n",
    "    # Calculate the mean and standard deviation of the tangency portfolio\n",
    "    tangency_return = np.dot(optimal_weights, mean_vector)\n",
    "    tangency_std = (np.dot(optimal_weights.T, np.dot(covariance_matrix, optimal_weights)))\n",
    "    \n",
    "    return {\n",
    "        'weights': optimal_weights,\n",
    "        'mean': tangency_return,\n",
    "        'std': tangency_std,\n",
    "        'maximized_value': -result.fun  # Negative because we minimized the negative of Sharpe ratio\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# Assuming RewardVector is a DataFrame containing your data\n",
    "\n",
    "result = tangency_weights(RewardVector.cov(), RewardVector.mean())\n",
    "print(\"Optimal Weights:\", result['weights'])\n",
    "print(\"Tangency Portfolio Mean:\", result['mean'] * 100)  # Assuming mean is annualized\n",
    "print(\"Tangency Portfolio Standard Deviation:\", result['std'])\n",
    "print(\"Maximized Value:\", result['maximized_value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay Buffer and Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size , input_shape, n_actions, discrete= True):\n",
    "        self.mem_size = max_size\n",
    "        self.input_spahe =input_shape\n",
    "        self.dis = discrete\n",
    "        self.mem_counter = 0\n",
    "        self.state_mem =np.zeros((self.mem_size , input_shape))\n",
    "        self.new_state_mem = np.zeros((self.mem_size , input_shape))\n",
    "\n",
    "        dtype = np.int16 if self.dis else np.float32\n",
    "        self.action_mem = np.zeros((self.mem_size , n_actions), dtype=dtype)\n",
    "        self.reward_mem = np.zeros((self.mem_size))\n",
    "        self.terminal_mem = np.zeros(self.mem_size, dtype=np.float32) #he has this as float???\n",
    "\n",
    "    def Store_transition(self, state, action , reward , next_state , done):\n",
    "        index = self.mem_counter%self.mem_size\n",
    "        self.state_mem[index] = state\n",
    "        self.new_state_mem[index] = next_state\n",
    "        self.action_mem[index] = action\n",
    "        self.reward_mem[index] = reward\n",
    "        self.terminal_mem[index] = 1- int(done)\n",
    "\n",
    "        if self.dis:\n",
    "            actions = np.zeros(self.action_mem.shape[1])\n",
    "            actions[action] =1.0\n",
    "            self.action_mem[index] = actions\n",
    "        else:\n",
    "            self.action_mem[index] = action\n",
    "        self.mem_counter+=1\n",
    "\n",
    "    def Sample_buffer(self, batch_size):\n",
    "        Max_mem = min(self.mem_counter , batch_size)\n",
    "        batch = np.random.choice(Max_mem, batch_size , replace= False)\n",
    "        states = self.state_mem[batch]\n",
    "        states_ = self.new_state_mem[batch]\n",
    "        action = self.action_mem[batch]\n",
    "        reward = self.reward_mem[batch]\n",
    "        EpisodeEnds = self.terminal_mem[batch]\n",
    "\n",
    "        return states, action, reward,states_, EpisodeEnds\n",
    "    \n",
    "class ContextualBandit(object):\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim,steps, transform = None, gamma=0.99, alpha=0.0001, epsilon=1, decay=0.99999,\n",
    "                  decaya=0.9 , cf1 = 64 , cf2 = 64, batch_size = 15000, max_mem = 750000, model_file = 'dqn_model.h5' , tau =0.05 , eta = 1e-2 ):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.maxmem = max_mem\n",
    "        self.steps = steps\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.model_file = model_file\n",
    "        self.eps = epsilon\n",
    "        self.decay = decay\n",
    "        self.decaya = decaya\n",
    "        self.actions = self.generate_grid(action_dim-1, self.steps, transform)\n",
    "        self.action_dim = len(self.actions)\n",
    "        self.tau = tau\n",
    "        self.cf1 = cf1\n",
    "        self.cf2 = cf2\n",
    "        self.last_vt = 0\n",
    "        self.last_wt = 0\n",
    "        self.eta = eta\n",
    "        self.last_sr = 0\n",
    "        self.count = 0\n",
    "        self.sr = 0\n",
    "        self.etas = 1\n",
    "        self.model = self.build_model()  # Initialize model after num_tilings\n",
    "        self.target = self.build_model()\n",
    "        self.Memory = ReplayBuffer(max_mem, state_dim, self.action_dim , discrete= True)\n",
    "        self.Update_NN_Parameters(tau=1)\n",
    "    \n",
    "    def generate_grid(self, n, m, offset=None):\n",
    "        # Generate all possible combinations of bin sizes\n",
    "        points = []\n",
    "        for combination in itertools.product(*[range(-m, m + 1) for _ in range(n)]):\n",
    "            if all(abs(x) <= m for x in combination) and all(-1 <= x / m <= 1 for x in combination):\n",
    "                points.append(combination)\n",
    "\n",
    "        points = np.array(points) / m\n",
    "        \n",
    "        if offset is not None:\n",
    "            points += np.array(offset)\n",
    "        \n",
    "        # Filter out points where the sum of absolute values is greater than 1\n",
    "        points = [point for point in points if sum(abs(x) for x in point) <= 1]\n",
    "        \n",
    "        return np.array(points)\n",
    "        \n",
    "    \n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential([\n",
    "            LSTM(self.cf1, activation='relu', input_shape=(None, self.state_dim)),\n",
    "            Dense(self.action_dim, activation='softmax')\n",
    "        ])\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.alpha))\n",
    "        return model\n",
    "\n",
    "        \n",
    "    def Remember(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        self.Memory.Store_transition(state,action, reward, next_state, done)\n",
    "    \n",
    "    def select_action(self, state, bool=True):\n",
    "        \n",
    "        if max(self.eps, 0.01) > np.random.random() and bool:\n",
    "            self.eps *= self.decay\n",
    "            return np.random.randint(self.action_dim)\n",
    "        else: \n",
    "            \n",
    "            action_probabilities = self.model.predict(state)\n",
    "            action_index = np.argmax(action_probabilities)\n",
    "            return action_index\n",
    "        \n",
    "    def Update_NN_Parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "            \n",
    "        actor_weights = self.model.get_weights()\n",
    "        actor_target_weights = self.target.get_weights()\n",
    "\n",
    "        updated_actor_target_weights = []\n",
    "\n",
    "        for actor_weight, target_weight in zip(actor_weights, actor_target_weights):\n",
    "            updated_weight = actor_weight * tau + target_weight * (1 - tau)\n",
    "            updated_actor_target_weights.append(updated_weight)\n",
    "\n",
    "        self.target.set_weights(updated_actor_target_weights)\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        if self.Memory.mem_counter < self.batch_size:\n",
    "            return 0, 0, 0  # Returning an additional 0 for TD error\n",
    "        \n",
    "        # Sample minibatch from replay buffer\n",
    "        state, action, reward, new_state, done = self.Memory.Sample_buffer(self.batch_size)\n",
    "        state = state.reshape(self.batch_size, 1, self.state_dim)\n",
    "        new_state = new_state.reshape(self.batch_size, 1, self.state_dim)\n",
    "        \n",
    "        # Compute Q-values for current and next states\n",
    "        q_eval = self.model.predict(state)\n",
    "        q_next = self.model.predict(new_state)\n",
    "        q_target = self.target.predict(state)\n",
    "        \n",
    "        # Compute target Q-values using the target network\n",
    "        q_next_target = self.target.predict(new_state)\n",
    "        \n",
    "        # Compute action indices\n",
    "        actionind = np.argmax(action, axis=1)\n",
    "        \n",
    "        # Define batch indices\n",
    "        batchindex = np.arange(self.batch_size)\n",
    "        \n",
    "        # Update target Q-values less frequently\n",
    "        if self.count % 450 == 0:\n",
    "            self.Update_NN_Parameters()\n",
    "        \n",
    "        # Calculate TD error\n",
    "        td_target = reward + self.gamma * np.max(q_next_target, axis=1) * (1 - done)\n",
    "        td_error = td_target - q_eval[batchindex, actionind]\n",
    "        \n",
    "        # Update Q-network\n",
    "        q_target[batchindex, actionind] = td_target\n",
    "        \n",
    "        # Train the main Q-network\n",
    "        _ = self.model.fit(state, q_target, verbose=0)\n",
    "        \n",
    "        # Compute Q-values after update\n",
    "        q_eval_after_update = self.model.predict(state)\n",
    "        self.alpha *= self.decaya\n",
    "        \n",
    "        return q_eval, q_eval_after_update, td_error\n",
    "    \n",
    "    def save_model(self):\n",
    "        self.model.save(self.model_file)\n",
    "\n",
    "    def _tiny(self):\n",
    "        return np.finfo('float64').eps\n",
    "    \n",
    "    \n",
    "    def calculate_dsr(self, rt):\n",
    "        self.sr = self.last_sr\n",
    "        if self.count < 1 / self.eta:\n",
    "            self.etas = 1/(self.count + 1)\n",
    "            changeA = (-self.last_vt + rt) / (self.count + 1)\n",
    "            changeB = (-self.last_wt + rt ** 2) / (self.count + 1)\n",
    "        else:\n",
    "            changeA =  self.eta * (rt - self.last_vt)\n",
    "            changeB =  self.eta * (rt ** 2 - self.last_wt)\n",
    "            self.etas = self.eta\n",
    "        if self.last_wt - self.last_vt ** 2 != 0:\n",
    "            self.last_sr = (self.last_wt*changeA/self.etas -0.5*self.last_vt*changeB/self.etas)/((self.last_wt-self.last_vt**2)**(1.5))\n",
    "        else:\n",
    "            self.last_sr = 0\n",
    "        self.count += 1\n",
    "        self.last_wt += changeB\n",
    "        self.last_vt += changeA\n",
    "        return self.last_sr \n",
    "    \n",
    "    def reset(self):\n",
    "        self.count = 0\n",
    "        self.last_sr = 0\n",
    "        self. last_vt = 0\n",
    "        self.last_wt = 0\n",
    "        self.etas = 1\n",
    "    def GetReward(self, action, rewards):\n",
    "        act = self.actions[action%(len(self.actions))]\n",
    "\n",
    "        if np.sum(np.abs(act)) >1:\n",
    "            return - 10000000000000\n",
    "        else:\n",
    "            if action < len(self.actions): #the last one is a buy\n",
    "                acti = np.append(act , 1-np.sum(np.abs(act)))\n",
    "            else: # For a sell of the last.\n",
    "                acti = np.append(act , np.sum(np.abs(act))-1)\n",
    "        \n",
    "        reward = np.array(rewards)\n",
    "        Rt = np.dot(reward,np.array(acti))\n",
    "        reward = self.calculate_dsr(Rt)\n",
    "        \n",
    "        return reward "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the two DataFrames along axis 1 (columns)\n",
    "df = InputStates\n",
    "\n",
    "# Optional: Reset the index of the concatenated DataFrame\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "state_dim = df.shape[1] + 3\n",
    "action_dim = RewardVector.shape[1]\n",
    "agent = ContextualBandit(state_dim, action_dim, alpha=0.00000001,steps=20, decay=0.999, batch_size=15000, eta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store evaluation results\n",
    "eval = []\n",
    "eval2 = []\n",
    "# Concatenate the two DataFrames along axis 1 (columns)\n",
    "df = InputStates\n",
    "\n",
    "# Optional: Reset the index of the concatenated DataFrame\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "state_dim = df.shape[1] + 3\n",
    "action_dim = RewardVector.shape[1]\n",
    "agent = ContextualBandit(state_dim, action_dim, alpha=0.000000001,steps=20, decay=0.999, batch_size=15000, eta=0.01)\n",
    "globalaction = 500 #example golbal action\n",
    "\n",
    "\n",
    "\n",
    "model_path = \"RealData3stocksV1.pkl\"\n",
    "\n",
    "save_interval = 500  # Save the model every 1000 iterations\n",
    "\n",
    "for j in range(50):\n",
    "    a = np.random.randint(110 , len(df)- 2000)\n",
    "    \n",
    "    agent.reset()\n",
    "    for i in range(100):\n",
    "        action = globalaction\n",
    "        agent.GetReward(action, RewardVector.iloc[i + a])\n",
    "    for i in range(500):\n",
    "        state = df.iloc[i + a]*100\n",
    "        state_array = state.values.reshape(1, -1)\n",
    "        result = np.hstack((state_array, np.array([[agent.last_vt, agent.last_wt, agent.etas]]).reshape(1, -1)))[0]\n",
    "        result = result.reshape(1, 1, df.shape[1]+3)\n",
    "        \n",
    "        action = agent.select_action(result, bool=True)\n",
    "        reward = agent.GetReward(action, RewardVector.iloc[i + a])\n",
    "        \n",
    "\n",
    "        next_state = df.iloc[i + a + 1]*100\n",
    "        next_state_array = next_state.values.reshape(1, -1)\n",
    "        result2 = np.hstack((next_state_array, np.array([[agent.last_vt, agent.last_wt, agent.etas]]).reshape(1, -1)))[0]\n",
    "        result2 = result2.reshape(1, 1, df.shape[1]+3)\n",
    "        done = False\n",
    "\n",
    "        if i % 50 == 0 or agent.count == 500:\n",
    "            agent.Remember(result, action, reward, result2, done)\n",
    "            e1, e2 , td = agent.learn()\n",
    "\n",
    "            if isinstance(e1, np.ndarray) and isinstance(e2, np.ndarray):\n",
    "                eval.append(np.mean(abs(td)))\n",
    "                eval2.append(np.mean(e2))\n",
    "            \n",
    "\n",
    "        agent.Remember(result, action, reward, result2, done)\n",
    "\n",
    "        if i % save_interval == 0:\n",
    "            # Save the model periodically\n",
    "            with open(model_path, 'wb') as f:\n",
    "                pickle.dump(agent, f)\n",
    "            print(f\"Model saved at iteration {i} as {model_path}\")\n",
    "\n",
    "plt.plot(eval)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Rewards')\n",
    "plt.title('Reward Trend over Training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verification Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_actions = []\n",
    "test_rewards = []\n",
    "rew = []\n",
    "df = InputStates\n",
    "# Testing loop\n",
    "agent.reset()\n",
    "a = 0\n",
    "for i in range(len(df) - 600, len(df)-500):\n",
    "    action = 178\n",
    "    agent.GetReward(action, RewardVector.iloc[i + a])\n",
    "\n",
    "for i in range(len(df) - 500, len(df)):\n",
    "    state = df.iloc[i + a]\n",
    "    state_array = state.values.reshape(1, -1)*100\n",
    "    result = np.hstack((state_array, np.array([[agent.last_vt, agent.last_wt, agent.etas]]).reshape(1, -1)))[0]\n",
    "    result = result.reshape(1, 1, df.shape[1] + 3)\n",
    "    action = agent.select_action(result, bool=True)\n",
    "    test_actions.append(action)\n",
    "    reward = agent.GetReward(action, RewardVector.iloc[i + a])\n",
    "    test_rewards.append(reward)\n",
    "    actie = agent.actions[action%len(agent.actions)]\n",
    "    if action < len(agent.actions): #the last one is a buy\n",
    "        acti = np.append(actie , 1-np.sum(np.abs(actie)))\n",
    "    else: # For a sell of the last.\n",
    "        acti = np.append(actie , np.sum(np.abs(actie))-1)\n",
    "        \n",
    "    Rt = np.dot(np.array(RewardVector.iloc[i + a]),np.array(acti))\n",
    "    #print(np.array(RewardVector.iloc[i + a]),np.array(acti))\n",
    "    rew.append(Rt)\n",
    "        \n",
    "\n",
    "# Convert test_rewards to a numpy array for calculations\n",
    "rew = np.array(rew)\n",
    "\n",
    "# Calculate Sharpe Ratio\n",
    "# Calculate average return\n",
    "average_return = np.mean(rew)\n",
    "\n",
    "risk_free_rate = 0\n",
    "\n",
    "# Calculate standard deviation of returns\n",
    "std_dev_returns = np.std(rew)\n",
    "\n",
    "# Compute the Sharpe Ratio\n",
    "sharpe_ratio = (average_return - risk_free_rate) / std_dev_returns\n",
    "\n",
    "# Convert log-normal returns to simple returns for cumulative calculations\n",
    "simple_returns = np.exp(rew) - 1\n",
    "\n",
    "# Compute cumulative returns\n",
    "cumulative_returns = np.cumprod(1 + simple_returns)\n",
    "\n",
    "# Calculate drawdowns\n",
    "peak = np.maximum.accumulate(cumulative_returns)\n",
    "drawdowns = (( cumulative_returns- peak) / peak)\n",
    "\n",
    "# Find the maximum drawdown\n",
    "max_drawdown = np.min(drawdowns)\n",
    "\n",
    "# Print results\n",
    "print(\"Actions taken during testing:\", test_actions)\n",
    "print(\"Rewards obtained during testing:\", test_rewards)\n",
    "print(f\"Sharpe Ratio: {sharpe_ratio:.4f}\")\n",
    "print(f\"Maximum Drawdown: {max_drawdown:.4f}\")\n",
    "\n",
    "# Plot rewards\n",
    "plt.plot(test_rewards)\n",
    "plt.title(\"Rewards Obtained During Testing\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()\n",
    "\n",
    "# Plot cumulative returns\n",
    "plt.plot(cumulative_returns)\n",
    "plt.title(\"Cumulative Returns\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Cumulative Return\")\n",
    "plt.show()\n",
    "\n",
    "# Plot drawdowns\n",
    "plt.plot(drawdowns)\n",
    "plt.title(\"Drawdowns\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Drawdown\")\n",
    "plt.show()\n",
    "\n",
    "from collections import Counter\n",
    "action_counts = Counter(test_actions)\n",
    "\n",
    "\n",
    "# Determine the five most common actions\n",
    "most_common_actions = action_counts.most_common(5)\n",
    "\n",
    "# Calculate the percentage of time each of these actions appears\n",
    "total_actions = len(test_actions)\n",
    "most_common_actions_percentages = [(action, count, (count / total_actions) * 100) for action, count in most_common_actions]\n",
    "\n",
    "print(most_common_actions_percentages)\n",
    "\n",
    "a1 =cumulative_returns\n",
    "a1draw = drawdowns"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
